{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHVjEp7uXYUp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CS 5480: Deep Learning - Homework 1 Coding Problems\n",
    "\n",
    "**Name:** Aaron Dubale\n",
    "\n",
    "**Student ID:** 12592130\n",
    "\n",
    "**Due Date:** February 17, 2026 at 11.59 PM\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Complete all TODOs in the code cells below\n",
    "- Add markdown cells with your analysis and interpretations where requested\n",
    "- Include all plots with proper labels, legends, and titles\n",
    "- Show all output (don't delete print statements)\n",
    "- Submit this completed notebook (.ipynb file)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utm6iyQIXYUr"
   },
   "source": [
    "# Problem 5: Bias-Variance Through Experimentation (30 points)\n",
    "\n",
    "## Objective\n",
    "\n",
    "Empirically observe the bias-variance tradeoff by fitting polynomials of different degrees to noisy data. You will see firsthand how model complexity affects both training and generalization performance.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Generate data from the function: $y = \\sin(2\\pi x) + \\epsilon$ where $\\epsilon \\sim N(0, 0.3^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvWMNg0mXYUr"
   },
   "source": [
    "## Part (a): Data Generation and Visualization (5 points)\n",
    "\n",
    "Generate training and test datasets, then visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0uvDikwXYUr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# True function\n",
    "def true_function(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "# Generate training data (small sample)\n",
    "n_train = 20\n",
    "X_train = np.random.uniform(0, 1, n_train)\n",
    "y_train = true_function(X_train) + np.random.normal(0, 0.3, n_train)\n",
    "\n",
    "# Generate test data (larger, for evaluation)\n",
    "n_test = 100\n",
    "X_test = np.random.uniform(0, 1, n_test)\n",
    "y_test = true_function(X_test) + np.random.normal(0, 0.3, n_test)\n",
    "\n",
    "# TODO: Create a plot showing:\n",
    "# 1. The true function (smooth curve from 0 to 1)\n",
    "# 2. Training points (scatter plot)\n",
    "# Label axes, add legend, and title\n",
    "\n",
    "x_plot = np.linspace(0, 1, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# TODO: Plot the true function\n",
    "# plt.plot(x_plot, true_function(x_plot), ...)\n",
    "\n",
    "# TODO: Plot training data points\n",
    "# plt.scatter(X_train, y_train, ...)\n",
    "\n",
    "# TODO: Add labels, legend, title\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orv_CddBXYUs"
   },
   "source": [
    "## Part (b): Fit Polynomials of Different Degrees (7 points)\n",
    "\n",
    "Fit polynomial models of degrees $d \\in \\{1, 3, 5, 9, 15\\}$ to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVCRXmWhXYUs"
   },
   "outputs": [],
   "source": [
    "# Fit polynomials of different degrees\n",
    "degrees = [1, 3, 5, 9, 15]\n",
    "models = {}\n",
    "\n",
    "for d in degrees:\n",
    "    # TODO: Fit polynomial of degree d using np.polyfit\n",
    "    # Hint: Store the result in the models dictionary\n",
    "    # coeffs = ...\n",
    "    # models[d] = ...\n",
    "\n",
    "    pass # Remove this pass after implementing\n",
    "\n",
    "# TODO: Create a figure with subplots (or one large plot)\n",
    "# For each degree, plot:\n",
    "# 1. Training data (scatter)\n",
    "# 2. True function (smooth curve)\n",
    "# 3. Fitted polynomial (smooth curve, use np.polyval)\n",
    "\n",
    "x_plot = np.linspace(0, 1, 200)\n",
    "\n",
    "# Option 1: Subplots (recommended for clarity)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # TODO: Plot on ax\n",
    "    # ax.scatter(X_train, y_train, ...)\n",
    "    # ax.plot(x_plot, true_function(x_plot), ...)\n",
    "    # y_pred = np.polyval(models[d], x_plot)\n",
    "    # ax.plot(x_plot, y_pred, ...)\n",
    "    # ax.set_title(f'Degree {d}')\n",
    "    # ax.legend()\n",
    "    pass\n",
    "\n",
    "# Remove the extra subplot\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: You should observe:\n",
    "# - Low degrees: smooth but miss the pattern (underfitting)\n",
    "# - Medium degrees: capture the pattern reasonably\n",
    "# - High degrees: wild oscillations between points (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ab7Du96XYUt"
   },
   "source": [
    "## Part (c): Compute Training and Test Errors (6 points)\n",
    "\n",
    "For each polynomial degree, compute the Mean Squared Error on both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrqTHyJPXYUt"
   },
   "outputs": [],
   "source": [
    "def compute_mse(X, y_true, coeffs):\n",
    "    \"\"\"Compute mean squared error for polynomial model\"\"\"\n",
    "    y_pred = np.polyval(coeffs, X)\n",
    "    mse = np.mean((y_pred - y_true) ** 2)\n",
    "    return mse\n",
    "\n",
    "train_errors = {}\n",
    "test_errors = {}\n",
    "\n",
    "for d in degrees:\n",
    "    # TODO: Compute train and test MSE for each degree\n",
    "    # 1. Use compute_mse() with models[d]\n",
    "    # 2. Store results in train_errors and test_errors dictionaries\n",
    "\n",
    "    # train_mse = ...\n",
    "    # test_mse = ...\n",
    "\n",
    "    # train_errors[d] = train_mse\n",
    "    # test_errors[d] = test_mse\n",
    "\n",
    "    pass # Remove this pass after implementing\n",
    "\n",
    "# TODO: Create a plot with degree on x-axis and MSE on y-axis\n",
    "# Plot both train error and test error on the same plot\n",
    "# Use different colors/markers for train vs test\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# TODO: Plot training and test errors\n",
    "# plt.plot(degrees, [train_errors[d] for d in degrees], ...)\n",
    "# plt.plot(degrees, [test_errors[d] for d in degrees], ...)\n",
    "\n",
    "# TODO: Add labels, legend, title, grid\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# TODO: You should observe:\n",
    "# - Training error decreases monotonically\n",
    "# - Test error: decreases initially, then increases (U-shaped curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8YId29jXYUt"
   },
   "source": [
    "## Part (d): Multiple Runs to Measure Variance (7 points)\n",
    "\n",
    "Repeat the entire experiment 50 times with different random training sets to measure how sensitive each model is to the specific training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0cuKIt1XYUt"
   },
   "outputs": [],
   "source": [
    "n_runs = 50\n",
    "test_mse_runs = {d: [] for d in degrees}\n",
    "\n",
    "# Use a fixed test set for all runs (for fair comparison)\n",
    "np.random.seed(123)\n",
    "X_test_fixed = np.random.uniform(0, 1, 100)\n",
    "y_test_fixed = true_function(X_test_fixed) + np.random.normal(0, 0.3, 100)\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # Generate new training data with different random seed\n",
    "    X_train_run = np.random.uniform(0, 1, n_train)\n",
    "    y_train_run = true_function(X_train_run) + np.random.normal(0, 0.3, n_train)\n",
    "\n",
    "    for d in degrees:\n",
    "        # TODO: Fit polynomial of degree d using the new training data (X_train_run, y_train_run)\n",
    "        # coeffs = ...\n",
    "\n",
    "        # TODO: Compute test MSE on the fixed test set (X_test_fixed, y_test_fixed)\n",
    "        # test_mse = ...\n",
    "\n",
    "        # TODO: Append the test_mse to the list for this degree\n",
    "        # test_mse_runs[d].append(test_mse)\n",
    "\n",
    "        pass\n",
    "\n",
    "# TODO: Compute mean and standard deviation of test MSE across runs for each degree\n",
    "# Hint: Use np.mean() and np.std() on the lists in test_mse_runs\n",
    "# mean_test_mse = ...\n",
    "# std_test_mse = ...\n",
    "\n",
    "# TODO: Create a plot with error bars\n",
    "# x-axis: polynomial degree\n",
    "# y-axis: mean test MSE\n",
    "# error bars: +/- 1 standard deviation\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# TODO: Use plt.errorbar to visualize the variance\n",
    "# plt.errorbar(...)\n",
    "\n",
    "# TODO: Add labels, title, grid\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print results (this part can stay or be commented out if you want them to write it)\n",
    "# print(\"\\nMean Test MSE across 50 runs:\")\n",
    "# for d in degrees:\n",
    "#     print(f\"Degree {d:2d}: {mean_test_mse[d]:.4f} +/- {std_test_mse[d]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR8Fs8GYXYUt"
   },
   "source": [
    "## Part (e): Analysis and Interpretation (5 points)\n",
    "\n",
    "Answer the following questions based on your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtTe3RWBXYUt"
   },
   "source": [
    "### (i) Which degree has the lowest training error on average? Does this surprise you?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXKOKTTDXYUt"
   },
   "source": [
    "### (ii) Which degree has the lowest test error on average? Is it the same as part (i)?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmBiqacNXYUt"
   },
   "source": [
    "### (iii) Describe the bias-variance tradeoff you observe\n",
    "\n",
    "**Low-degree polynomials (e.g., degree 1):**\n",
    "\n",
    "**High-degree polynomials (e.g., degree 15):**\n",
    "\n",
    "**Best tradeoff (which degree?):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXQCvQGPXYUt"
   },
   "source": [
    "### (iv) The variance across runs tells us how sensitive the model is to the specific training data. Which models have the highest variance? Why does this make sense?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmILEQE6XYUu"
   },
   "source": [
    "### (v) If you had to choose a model for deployment (where you can't see the test set), how would you make the decision? What does this tell you about the importance of validation sets?\n",
    "\n",
    "**Your answer:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
