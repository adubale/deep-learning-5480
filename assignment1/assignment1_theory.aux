\relax 
\providecommand*\new@tpo@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Understanding Learning as Function Approximation}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Counting Model Capacity}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Model Capacity and Real-World Constraints}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Why Training Error Misleads You}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Loss Functions and Their Properties}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Computing Different Losses}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Outlier Sensitivity in Production Systems}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Why Gradient Descent Needs Smooth Losses}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Why Stacking Linear Layers Does Nothing}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Two Linear Layers}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}With Nonlinearity}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Sigmoid and Tanh Equivalence}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Sigmoid in Exponential Form}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Scaling the Input}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Deriving the Tanh Relationship}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Neural Network Implications}{2}{}\protected@file@percent }
\gdef \@abspage@last{2}
